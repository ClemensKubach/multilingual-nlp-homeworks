echo " MNLP2025 Cultural Classifier â€“ Rule-Based Approach

This is a group project for the MNLP 2025 course, aiming to classify cultural items into one of three categories:

- **Cultural Agnostic**
- **Cultural Representative**
- **Cultural Exclusive**

The classification is based on enriched knowledge from **Wikidata** and a custom-built rule-based classifier.

---

## ðŸ“ Project Structure

\`\`\`
MNLP2025_Cultural_Classifier/
â”œâ”€â”€ data/                       # CSV files (original, processed, and enriched)
â”‚   â”œâ”€â”€ validation_sapienza.csv
â”‚   â”œâ”€â”€ validation_prepared.csv
â”‚   â”œâ”€â”€ validation_enriched.csv
â”‚   â”œâ”€â”€ rule_predictions.csv
â”‚   â””â”€â”€ train_sapienza.csv      # Optional, not currently used
â”œâ”€â”€ diagnostics/                # Diagnostic tools for error analysis
â”‚   â””â”€â”€ frequent_instance_qids.py
â”œâ”€â”€ scripts/                    # Main pipeline and supporting tools
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ diagnostics.py
â”‚   â”œâ”€â”€ enrichment_coverage.py
â”‚   â”œâ”€â”€ inspect_misclassified.py
â”‚   â””â”€â”€ diagnostics_analyze_exclusive.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
\`\`\`

---

## ðŸ§© Workflow Overview

### 1. **Data Preparation**
- Extract QIDs from \`validation_sapienza.csv\`
- Output: \`validation_prepared.csv\`

### 2. **Data Enrichment**
- For each QID, fetch data from Wikidata using the SPARQL API:
  - \`instance_of\`, \`part_of_culture\`, \`heritage_status\`, etc.
- Output: \`validation_enriched.csv\`

### 3. **Rule-Based Classification**
- Uses manually designed logic based on common \`instance_of\` and \`heritage_status\` values.
- The main classifier logic is in \`main.py\`.
- Output: \`rule_predictions.csv\`

### 4. **Evaluation**
- Calculates accuracy, classification report, and confusion matrix.
- Displays label distribution (GT vs Prediction) and highlights imbalance.
- Run diagnostics from \`diagnostics.py\`

---

## ðŸ§ª How to Run the Project

### ðŸ”§ Setup
\`\`\`bash
pip install -r requirements.txt
\`\`\`

### â–¶ï¸ Run the Classifier
\`\`\`bash
python scripts/main.py
\`\`\`

### ðŸ“Š Run Diagnostics
\`\`\`bash
python scripts/diagnostics.py
\`\`\`

---

## âœ… Results So Far

- Accuracy: ~ **56%**
- Strong precision on **Cultural Agnostic**
- Improving **Cultural Representative** and **Cultural Exclusive** using targeted QID-based rule tuning

---

## ðŸš§ Next Steps (for team continuation)

- Add more \`instance_of\` and \`part_of_culture\` QIDs to improve underperforming classes
- Explore adding multilingual label support (e.g., using aliases from Wikidata)
- Combine with LM-based approach for hybrid prediction

---

## ðŸ™Œ Contributors
- Joshua Edwin, Clemens Kubach(teXt-Men)

Feel free to continue this project and improve the rule-based logic!

---
" > README.md
